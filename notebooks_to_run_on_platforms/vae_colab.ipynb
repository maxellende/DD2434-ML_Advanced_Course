{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOG8Mlr4npAPaj/Lo8h5xjL"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "\n",
    "mount_path = '/content/drive/'\n",
    "drive.mount(mount_path)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YZ0o7nfThRo2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543766058,
     "user_tz": -60,
     "elapsed": 1679,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "4a4d7019-7231-4ff5-8fce-d93620b50009"
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# cd command has to start with % sign, cd into folder with all relevant files\n",
    "%cd \"drive/My Drive/dd2434-vae\"\n",
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mi1ISPUjW0B7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543767965,
     "user_tz": -60,
     "elapsed": 169,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "33b0935d-5cab-4fc6-abf1-c6bba8d2fd0c"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/My Drive/dd2434-vae\n",
      "checkpoints   main.py\t       README.md\t vae_colab.ipynb\n",
      "data_mnist    main.zip\t       requirements.txt  VAE.ipynb\n",
      "figures       plot_figures.py  test.py\t\t VAE.py\n",
      "logging_info  __pycache__      utils.py\t\t vae_test.py\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install torch torchvision torchaudio matplotlib numpy"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdsZXP1gUk8k",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543773047,
     "user_tz": -60,
     "elapsed": 3366,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "62d2b294-aeab-43aa-f3fb-82537f1a1fca"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.14.0+cu116)\n",
      "Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.13.0+cu116)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.12.7)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "vf_I4KZBMyMx",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543776653,
     "user_tz": -60,
     "elapsed": 1546,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import utils\n",
    "from time import time\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "import os\n",
    "from VAE import VAE\n",
    "\n",
    "from plot_figures import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# only needs to do this once, comment this out when it's done. It unzips the project into your google drive folder named dd2434-vae\n",
    "# !unzip \"/content/drive/My Drive/dd2434-vae/main.zip\" -d \"/content/drive/My Drive/dd2434-vae\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYa9ItHxOcde",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672525159640,
     "user_tz": -60,
     "elapsed": 2112,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "2121ded9-d694-475a-91bf-a68ae3d1db08"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Archive:  /content/drive/My Drive/dd2434-vae/main.zip\n",
      "replace /content/drive/My Drive/dd2434-vae/checkpoints/checkpoint_epoch_0_iter_0? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sys.path.append(os.path.join(mount_path, \"My Drive\", \"dd2434-vae\"))"
   ],
   "metadata": {
    "id": "dIkcWT2UT2eD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543811983,
     "user_tz": -60,
     "elapsed": 173,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sys.platform"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "yAvnF2SmViF7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543812960,
     "user_tz": -60,
     "elapsed": 4,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "491b91e5-7d99-4bc1-828b-f651b69fdf60"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'linux'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 6
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# if you're not using gpu to run the machine then it nvidia-smi won't work\n",
    "!nvidia-smi\n",
    "!python --version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TK5lTmB2NYIK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543815531,
     "user_tz": -60,
     "elapsed": 1012,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "2209091d-97d4-4a92-9e9d-d36aadf19024"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sun Jan  1 03:30:16 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P0    25W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n",
      "Python 3.8.16\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Device\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# paths\n",
    "PATH_TRAINING = \"checkpoints\"\n",
    "DATA_PATH = \"data_mnist\"\n",
    "LOGGING_PATH = \"logging_info\"\n",
    "if not os.path.exists(PATH_TRAINING):\n",
    "    os.makedirs(PATH_TRAINING)\n",
    "if not os.path.exists(LOGGING_PATH):\n",
    "    os.makedirs(LOGGING_PATH)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdhfgRB9PAie",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543821776,
     "user_tz": -60,
     "elapsed": 169,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "0ebe4bef-fa0e-417f-cd0b-315403f2df86"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import utils\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "import os\n",
    "from VAE import VAE\n",
    "\n",
    "from plot_figures import *\n",
    "\n",
    "# Device\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# paths\n",
    "PATH_TRAINING = \"checkpoints\"\n",
    "DATA_PATH = \"data_mnist\"\n",
    "LOGGING_PATH = \"logging_info\"\n",
    "if not os.path.exists(PATH_TRAINING):\n",
    "    os.makedirs(PATH_TRAINING)\n",
    "if not os.path.exists(LOGGING_PATH):\n",
    "    os.makedirs(LOGGING_PATH)\n",
    "\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, test_loader, params, train_settings, state):\n",
    "    # number of steps or epoch not given in the paper so have to experiment with\n",
    "    # 60k training data from MNIST, 100 mini batches\n",
    "    training_losses = []\n",
    "    datapoint_training_losses = []\n",
    "    test_losses = []\n",
    "    datapoint_test_losses = []\n",
    "\n",
    "    if train_settings.load_checkpoint:\n",
    "        datapoint_training_losses = state['datapoint_training_losses']\n",
    "        training_losses = state['training_losses']\n",
    "        datapoint_test_losses = state['datapoint_test_losses']\n",
    "        test_losses = state['test_losses']\n",
    "\n",
    "    logging.info(\"Start training...\")\n",
    "    for epoch in range(train_settings.start_epoch, params.NUM_EPOCHS+1):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            # Forward and back prop\n",
    "            x = x.to(DEVICE)\n",
    "            _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "\n",
    "            loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # save model checkpoints\n",
    "            if epoch % train_settings.save_rate_epoch == 0 and i % train_settings.save_rate_iter == 0 and train_settings.save_checkpoint:\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'datapoint_training_losses': datapoint_training_losses,\n",
    "                    'training_losses': training_losses,\n",
    "                    'datapoint_test_losses': datapoint_test_losses,\n",
    "                    'test_losses': test_losses,\n",
    "                    # the loss may not be needed since we start over from a new epoch when resuming\n",
    "                }\n",
    "                # logging.info(f\"Saving model checkpoint at epoch: {epoch}, iter: {i}\")\n",
    "                # logging.info(f\"state_dict: {model.state_dict()}, optimizer: {optimizer.state_dict()}, loss: {loss}\")\n",
    "                torch.save(state, os.path.join(PATH_TRAINING, f'checkpoint_epoch_{epoch}_iter_{i}.pt'))\n",
    "\n",
    "            if i % train_settings.track_rate == 0:\n",
    "                logging.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, i * len(x), len(train_loader.dataset),\n",
    "                    100. * i / len(train_loader),\n",
    "                    loss.item() / len(x)))\n",
    "\n",
    "            datapoint_training_losses.append(loss.item() / len(x))\n",
    "\n",
    "            datapoint_test_loss = eval(model, test_loader, loss_fn, True)\n",
    "            datapoint_test_losses.append(datapoint_test_loss)\n",
    "\n",
    "        training_loss = running_loss / len(train_loader.dataset)\n",
    "        training_losses.append(training_loss)\n",
    "        logging.info('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "            epoch, training_loss))\n",
    "\n",
    "        test_loss = eval(model, test_loader, loss_fn, None)\n",
    "        test_losses.append(test_loss)\n",
    "        logging.info('====> Epoch: {} Average testing loss: {:.4f}'.format(\n",
    "            epoch, test_loss))\n",
    "\n",
    "    logging.info(f\"params: {str(params.__dict__)}, train settings: {str(train_settings.__dict__)}\")\n",
    "\n",
    "    return training_losses, datapoint_training_losses, test_losses, datapoint_test_losses\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, test_loader, loss_fn, stop=None):\n",
    "    \"\"\"\n",
    "    Train on the test dataset\n",
    "    Can be used to test model after one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    datapoint_test_losses = []\n",
    "    running_loss = 0\n",
    "\n",
    "    if not stop:  # evaluate model on whole test dataset\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            # Forward and back prop\n",
    "            x = x.to(DEVICE)\n",
    "            _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "\n",
    "            loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "            running_loss += loss.item()\n",
    "        return running_loss / len(test_loader.dataset)\n",
    "\n",
    "    if stop:  # evaluate model on only one batch\n",
    "        it = iter(test_loader)\n",
    "        x, c = next(it)\n",
    "        x = x.to(DEVICE)\n",
    "        _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "\n",
    "        loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "\n",
    "        return loss.item() / len(x)\n",
    "\n",
    "\n",
    "def test_image(model, x, num_epochs, latent_dim, stop=None, loss_fn=None):\n",
    "    if not loss_fn:\n",
    "        loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "    loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "    x = x[0]\n",
    "    reconstructed_x = reconstructed_x.detach().numpy().reshape(x.shape)\n",
    "    save_test_image(x, reconstructed_x, loss.item(), num_epochs, latent_dim)\n",
    "\n",
    "\n",
    "def resume_training(model, optimizer, file_name):\n",
    "    state = torch.load(os.path.join(PATH_TRAINING, file_name))\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    epoch = state['epoch']\n",
    "    # logging.info(f\"Loading model checkpoint\")\n",
    "    # logging.info(f\"state_dict: {model.state_dict()}, optimizer: {optimizer.state_dict()}, epoch: {epoch}\")\n",
    "    return epoch, optimizer, model, state\n",
    "\n",
    "\n",
    "def calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn):\n",
    "    # analytical form of -KL(q_fi(z|x) || p_theta(z))\n",
    "    kl_div = -0.5 * torch.sum(1 + z_log_var\n",
    "                              - z_mean ** 2\n",
    "                              - torch.exp(z_log_var))\n",
    "\n",
    "    # There are some motivations as to why we use MSE:\n",
    "    # https://stats.stackexchange.com/questions/347378/variational-autoencoder-why-reconstruction-term-is-same-to-square-loss\n",
    "    # if data is continuous then the decoder and encoder are gaussian according to the paper, we set p(x|z) to gaussian and get the following\n",
    "    # log(P(x | z)) \\propto log[e^(-|x-x'|^2)] \\propto |x-x'|^2\n",
    "    # others use binary cross-entropy which seems to give results closer to the paper\n",
    "    loss_log_likelihood = loss_fn(reconstructed_x, torch.flatten(x, start_dim=1))\n",
    "\n",
    "    return kl_div + loss_log_likelihood\n",
    "\n",
    "\n",
    "def main():\n",
    "    RANDOM_SEED = 123\n",
    "\n",
    "    # ugly solution perhaps, but needed to bind them to self in order to use in-built .__dict__ formatting to logging\n",
    "    class Params:\n",
    "        def __init__(self):\n",
    "            self.LEARNING_RATE = 2e-2\n",
    "            self.BATCH_SIZE = 100\n",
    "            self.NUM_EPOCHS = 1667\n",
    "            self.HIDDEN_DIMEN = 500\n",
    "            self.LATENT_SPACE = 3  # {3,5,10,20,200}\n",
    "\n",
    "    class RunningSettings:\n",
    "        def __init__(self):\n",
    "            self.optimizer = 'adagrad'  # adagrad or adam\n",
    "            self.criterion = 'bce'  # l1, mse or bce\n",
    "            self.hyperparam_search = False\n",
    "            self.train = True\n",
    "            self.plot = True\n",
    "            self.save_checkpoint = True\n",
    "            self.load_checkpoint = False\n",
    "            self.load_path = '' if not self.load_checkpoint else \"checkpoint_epoch_5_iter_599.pt\"\n",
    "            self.logging_filename = 'train.log'  # 'hyperparameter_search1.log'\n",
    "            self.track_rate = 300  # how often to log batch data loss\n",
    "            self.save_rate_epoch = 5  # how often to save model checkpoints per epoch\n",
    "            self.save_rate_iter = 600  # how often to save model checkpoint per batch iterations\n",
    "            self.start_epoch = 1  # where training epochs start, if loaded from checkpoint then it will be higher than 1\n",
    "            self.device = DEVICE\n",
    "\n",
    "    params = Params()\n",
    "    running_settings = RunningSettings()\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "    # setup logger so can save logging.infos, call logging.info\n",
    "    utils.set_logger(os.path.join(LOGGING_PATH, running_settings.logging_filename))\n",
    "\n",
    "    train_loader, test_loader = utils.create_dataset(data_path=DATA_PATH, batch_size=params.BATCH_SIZE)\n",
    "    MNIST_shape = train_loader.dataset.data.shape[1:]  # exclude batch dimension\n",
    "\n",
    "    model = utils.create_model(mnist_shape=MNIST_shape, hidden_dimen=params.HIDDEN_DIMEN,\n",
    "                               latent_space=params.LATENT_SPACE, device=DEVICE)\n",
    "    criterion = utils.create_criterion(running_settings.criterion)\n",
    "    optimizer = utils.create_optimizer(model=model, learning_rate=params.LEARNING_RATE, type=running_settings.optimizer)\n",
    "\n",
    "    # load model checkpoint\n",
    "    state = {}\n",
    "    if running_settings.load_checkpoint:\n",
    "        running_settings.start_epoch, optimizer, model, state = resume_training(model, optimizer, running_settings.load_path)\n",
    "\n",
    "    # hyperparameter search\n",
    "    if running_settings.hyperparam_search:\n",
    "        params_to_optimize = {'lr': [0.01, 0.02, 0.1], 'ls': [3, 5, 10, 20, 200]}\n",
    "        best_learning_rates = utils.learning_rate_hyperparam_search(parameters_to_optimize=params_to_optimize, params=params,\n",
    "                                                                    train_fn=train, mnist_shape=MNIST_shape,\n",
    "                                                                    train_loader=train_loader, test_loader=test_loader,\n",
    "                                                                    running_settings=running_settings)\n",
    "        logging.info(f\"Best learning rates for different latent spaces: {str(best_learning_rates)}\")\n",
    "\n",
    "    # train\n",
    "    if running_settings.train:\n",
    "        tic = time()\n",
    "        training_losses, datapoint_training_losses, test_losses, datapoint_test_losses = \\\n",
    "            train(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  train_loader=train_loader,\n",
    "                  test_loader=test_loader,\n",
    "                  loss_fn=criterion,\n",
    "                  params=params,\n",
    "                  train_settings=running_settings,\n",
    "                  state=state)\n",
    "        final_time = time() - tic\n",
    "        logging.info('Done (t={:0.2f}m)'.format(final_time / 60))\n",
    "\n",
    "    # plot evolution of loss along epochs/datapoint\n",
    "    if running_settings.plot:\n",
    "        plot_epoch_losses(training_losses, test_losses, params.LATENT_SPACE, params.NUM_EPOCHS)\n",
    "        plot_datapoint_losses(datapoint_training_losses, datapoint_test_losses, params.LATENT_SPACE, params.NUM_EPOCHS)\n",
    "\n",
    "        # run model on one image to test\n",
    "        # inputs, classes = next(iter(test_loader))\n",
    "        # x = inputs[np.random.randint(len(inputs))]\n",
    "        # test_image(model, x, params.LATENT_SPACE, params.NUM_EPOCHS, stop=None, loss_fn=criterion)\n",
    "        #\n",
    "        # # plot manifold for latent dimension of 2\n",
    "        # plot_manifold(model, DEVICE, n=12)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8T6wMoXPgJ9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543906723,
     "user_tz": -60,
     "elapsed": 287,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "0f0ba4d0-2890-4661-a2eb-2bbe2eee90d0"
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_upOd2S2eEqm",
    "outputId": "0fd7ae03-e7b4-4f0d-f4e4-e8ec402b08d1"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Start training...\n",
      "INFO:root:Train Epoch: 1 [0/60000 (0%)]\tLoss: 624.110234\n",
      "INFO:root:Train Epoch: 1 [30000/60000 (50%)]\tLoss: 185.663828\n",
      "INFO:root:====> Epoch: 1 Average training loss: 189.8297\n",
      "INFO:root:====> Epoch: 1 Average testing loss: 170.2856\n",
      "INFO:root:Train Epoch: 2 [0/60000 (0%)]\tLoss: 181.597559\n",
      "INFO:root:Train Epoch: 2 [30000/60000 (50%)]\tLoss: 162.175225\n",
      "INFO:root:====> Epoch: 2 Average training loss: 168.1929\n",
      "INFO:root:====> Epoch: 2 Average testing loss: 165.1539\n",
      "INFO:root:Train Epoch: 3 [0/60000 (0%)]\tLoss: 159.947832\n",
      "INFO:root:Train Epoch: 3 [30000/60000 (50%)]\tLoss: 159.558506\n",
      "INFO:root:====> Epoch: 3 Average training loss: 164.0277\n",
      "INFO:root:====> Epoch: 3 Average testing loss: 162.0854\n",
      "INFO:root:Train Epoch: 4 [0/60000 (0%)]\tLoss: 159.188838\n",
      "INFO:root:Train Epoch: 4 [30000/60000 (50%)]\tLoss: 156.317891\n",
      "INFO:root:====> Epoch: 4 Average training loss: 161.1350\n",
      "INFO:root:====> Epoch: 4 Average testing loss: 159.3753\n",
      "INFO:root:Train Epoch: 5 [0/60000 (0%)]\tLoss: 155.646592\n",
      "INFO:root:Train Epoch: 5 [30000/60000 (50%)]\tLoss: 156.057402\n",
      "INFO:root:====> Epoch: 5 Average training loss: 158.8845\n",
      "INFO:root:====> Epoch: 5 Average testing loss: 157.5342\n",
      "INFO:root:Train Epoch: 6 [0/60000 (0%)]\tLoss: 157.587949\n",
      "INFO:root:Train Epoch: 6 [30000/60000 (50%)]\tLoss: 145.317168\n",
      "INFO:root:====> Epoch: 6 Average training loss: 157.0124\n",
      "INFO:root:====> Epoch: 6 Average testing loss: 155.7246\n",
      "INFO:root:Train Epoch: 7 [0/60000 (0%)]\tLoss: 160.713730\n",
      "INFO:root:Train Epoch: 7 [30000/60000 (50%)]\tLoss: 160.316543\n",
      "INFO:root:====> Epoch: 7 Average training loss: 155.4817\n",
      "INFO:root:====> Epoch: 7 Average testing loss: 154.1593\n",
      "INFO:root:Train Epoch: 8 [0/60000 (0%)]\tLoss: 150.013369\n",
      "INFO:root:Train Epoch: 8 [30000/60000 (50%)]\tLoss: 156.723848\n",
      "INFO:root:====> Epoch: 8 Average training loss: 154.1581\n",
      "INFO:root:====> Epoch: 8 Average testing loss: 153.2371\n",
      "INFO:root:Train Epoch: 9 [0/60000 (0%)]\tLoss: 150.093584\n",
      "INFO:root:Train Epoch: 9 [30000/60000 (50%)]\tLoss: 158.286445\n",
      "INFO:root:====> Epoch: 9 Average training loss: 153.0556\n",
      "INFO:root:====> Epoch: 9 Average testing loss: 152.0796\n",
      "INFO:root:Train Epoch: 10 [0/60000 (0%)]\tLoss: 161.770049\n",
      "INFO:root:Train Epoch: 10 [30000/60000 (50%)]\tLoss: 158.670840\n",
      "INFO:root:====> Epoch: 10 Average training loss: 152.0882\n",
      "INFO:root:====> Epoch: 10 Average testing loss: 151.1943\n",
      "INFO:root:Train Epoch: 11 [0/60000 (0%)]\tLoss: 141.605928\n",
      "INFO:root:Train Epoch: 11 [30000/60000 (50%)]\tLoss: 144.728760\n",
      "INFO:root:====> Epoch: 11 Average training loss: 151.2930\n",
      "INFO:root:====> Epoch: 11 Average testing loss: 150.6783\n",
      "INFO:root:Train Epoch: 12 [0/60000 (0%)]\tLoss: 150.062012\n",
      "INFO:root:Train Epoch: 12 [30000/60000 (50%)]\tLoss: 149.928662\n",
      "INFO:root:====> Epoch: 12 Average training loss: 150.4877\n",
      "INFO:root:====> Epoch: 12 Average testing loss: 150.0248\n",
      "INFO:root:Train Epoch: 13 [0/60000 (0%)]\tLoss: 154.067236\n",
      "INFO:root:Train Epoch: 13 [30000/60000 (50%)]\tLoss: 150.901904\n",
      "INFO:root:====> Epoch: 13 Average training loss: 149.8944\n",
      "INFO:root:====> Epoch: 13 Average testing loss: 149.0121\n",
      "INFO:root:Train Epoch: 14 [0/60000 (0%)]\tLoss: 146.779922\n",
      "INFO:root:Train Epoch: 14 [30000/60000 (50%)]\tLoss: 151.696338\n",
      "INFO:root:====> Epoch: 14 Average training loss: 149.3421\n",
      "INFO:root:====> Epoch: 14 Average testing loss: 148.6642\n",
      "INFO:root:Train Epoch: 15 [0/60000 (0%)]\tLoss: 149.265117\n",
      "INFO:root:Train Epoch: 15 [30000/60000 (50%)]\tLoss: 150.532959\n",
      "INFO:root:====> Epoch: 15 Average training loss: 148.8162\n",
      "INFO:root:====> Epoch: 15 Average testing loss: 148.1230\n",
      "INFO:root:Train Epoch: 16 [0/60000 (0%)]\tLoss: 149.448799\n",
      "INFO:root:Train Epoch: 16 [30000/60000 (50%)]\tLoss: 151.756230\n",
      "INFO:root:====> Epoch: 16 Average training loss: 148.3461\n",
      "INFO:root:====> Epoch: 16 Average testing loss: 147.6520\n",
      "INFO:root:Train Epoch: 17 [0/60000 (0%)]\tLoss: 149.226396\n",
      "INFO:root:Train Epoch: 17 [30000/60000 (50%)]\tLoss: 150.987197\n",
      "INFO:root:====> Epoch: 17 Average training loss: 147.8971\n",
      "INFO:root:====> Epoch: 17 Average testing loss: 147.6279\n",
      "INFO:root:Train Epoch: 18 [0/60000 (0%)]\tLoss: 146.387129\n",
      "INFO:root:Train Epoch: 18 [30000/60000 (50%)]\tLoss: 149.545430\n",
      "INFO:root:====> Epoch: 18 Average training loss: 147.5231\n",
      "INFO:root:====> Epoch: 18 Average testing loss: 147.0591\n",
      "INFO:root:Train Epoch: 19 [0/60000 (0%)]\tLoss: 149.725977\n",
      "INFO:root:Train Epoch: 19 [30000/60000 (50%)]\tLoss: 142.278555\n",
      "INFO:root:====> Epoch: 19 Average training loss: 147.1469\n",
      "INFO:root:====> Epoch: 19 Average testing loss: 147.2655\n",
      "INFO:root:Train Epoch: 20 [0/60000 (0%)]\tLoss: 147.715488\n",
      "INFO:root:Train Epoch: 20 [30000/60000 (50%)]\tLoss: 142.869219\n",
      "INFO:root:====> Epoch: 20 Average training loss: 146.8455\n",
      "INFO:root:====> Epoch: 20 Average testing loss: 146.6166\n",
      "INFO:root:Train Epoch: 21 [0/60000 (0%)]\tLoss: 147.018926\n",
      "INFO:root:Train Epoch: 21 [30000/60000 (50%)]\tLoss: 149.994687\n",
      "INFO:root:====> Epoch: 21 Average training loss: 146.5436\n",
      "INFO:root:====> Epoch: 21 Average testing loss: 146.1566\n",
      "INFO:root:Train Epoch: 22 [0/60000 (0%)]\tLoss: 151.942959\n",
      "INFO:root:Train Epoch: 22 [30000/60000 (50%)]\tLoss: 147.604258\n",
      "INFO:root:====> Epoch: 22 Average training loss: 146.2192\n",
      "INFO:root:====> Epoch: 22 Average testing loss: 146.0562\n",
      "INFO:root:Train Epoch: 23 [0/60000 (0%)]\tLoss: 142.151553\n",
      "INFO:root:Train Epoch: 23 [30000/60000 (50%)]\tLoss: 151.998379\n",
      "INFO:root:====> Epoch: 23 Average training loss: 145.9747\n",
      "INFO:root:====> Epoch: 23 Average testing loss: 145.5710\n",
      "INFO:root:Train Epoch: 24 [0/60000 (0%)]\tLoss: 136.083564\n",
      "INFO:root:Train Epoch: 24 [30000/60000 (50%)]\tLoss: 149.138330\n",
      "INFO:root:====> Epoch: 24 Average training loss: 145.6947\n",
      "INFO:root:====> Epoch: 24 Average testing loss: 145.4745\n",
      "INFO:root:Train Epoch: 25 [0/60000 (0%)]\tLoss: 139.556582\n",
      "INFO:root:Train Epoch: 25 [30000/60000 (50%)]\tLoss: 138.649941\n",
      "INFO:root:====> Epoch: 25 Average training loss: 145.4681\n",
      "INFO:root:====> Epoch: 25 Average testing loss: 145.2268\n",
      "INFO:root:Train Epoch: 26 [0/60000 (0%)]\tLoss: 142.112031\n",
      "INFO:root:Train Epoch: 26 [30000/60000 (50%)]\tLoss: 143.158574\n",
      "INFO:root:====> Epoch: 26 Average training loss: 145.2052\n",
      "INFO:root:====> Epoch: 26 Average testing loss: 144.9145\n",
      "INFO:root:Train Epoch: 27 [0/60000 (0%)]\tLoss: 148.941611\n",
      "INFO:root:Train Epoch: 27 [30000/60000 (50%)]\tLoss: 141.112305\n",
      "INFO:root:====> Epoch: 27 Average training loss: 144.9593\n",
      "INFO:root:====> Epoch: 27 Average testing loss: 146.1814\n",
      "INFO:root:Train Epoch: 28 [0/60000 (0%)]\tLoss: 151.849277\n",
      "INFO:root:Train Epoch: 28 [30000/60000 (50%)]\tLoss: 146.266836\n",
      "INFO:root:====> Epoch: 28 Average training loss: 144.7663\n",
      "INFO:root:====> Epoch: 28 Average testing loss: 144.5323\n",
      "INFO:root:Train Epoch: 29 [0/60000 (0%)]\tLoss: 153.552158\n",
      "INFO:root:Train Epoch: 29 [30000/60000 (50%)]\tLoss: 146.570723\n",
      "INFO:root:====> Epoch: 29 Average training loss: 144.5230\n",
      "INFO:root:====> Epoch: 29 Average testing loss: 144.4032\n",
      "INFO:root:Train Epoch: 30 [0/60000 (0%)]\tLoss: 149.014248\n",
      "INFO:root:Train Epoch: 30 [30000/60000 (50%)]\tLoss: 150.942891\n",
      "INFO:root:====> Epoch: 30 Average training loss: 144.3294\n",
      "INFO:root:====> Epoch: 30 Average testing loss: 144.3092\n",
      "INFO:root:Train Epoch: 31 [0/60000 (0%)]\tLoss: 137.808105\n",
      "INFO:root:Train Epoch: 31 [30000/60000 (50%)]\tLoss: 148.682559\n",
      "INFO:root:====> Epoch: 31 Average training loss: 144.1308\n",
      "INFO:root:====> Epoch: 31 Average testing loss: 144.1509\n",
      "INFO:root:Train Epoch: 32 [0/60000 (0%)]\tLoss: 135.805586\n",
      "INFO:root:Train Epoch: 32 [30000/60000 (50%)]\tLoss: 144.672578\n",
      "INFO:root:====> Epoch: 32 Average training loss: 143.9444\n",
      "INFO:root:====> Epoch: 32 Average testing loss: 143.9210\n",
      "INFO:root:Train Epoch: 33 [0/60000 (0%)]\tLoss: 139.082129\n",
      "INFO:root:Train Epoch: 33 [30000/60000 (50%)]\tLoss: 144.010625\n",
      "INFO:root:====> Epoch: 33 Average training loss: 143.7727\n",
      "INFO:root:====> Epoch: 33 Average testing loss: 143.8382\n",
      "INFO:root:Train Epoch: 34 [0/60000 (0%)]\tLoss: 149.315547\n",
      "INFO:root:Train Epoch: 34 [30000/60000 (50%)]\tLoss: 132.142705\n",
      "INFO:root:====> Epoch: 34 Average training loss: 143.5906\n",
      "INFO:root:====> Epoch: 34 Average testing loss: 143.6927\n",
      "INFO:root:Train Epoch: 35 [0/60000 (0%)]\tLoss: 141.470293\n",
      "INFO:root:Train Epoch: 35 [30000/60000 (50%)]\tLoss: 143.458096\n",
      "INFO:root:====> Epoch: 35 Average training loss: 143.4249\n",
      "INFO:root:====> Epoch: 35 Average testing loss: 143.8349\n",
      "INFO:root:Train Epoch: 36 [0/60000 (0%)]\tLoss: 149.656387\n",
      "INFO:root:Train Epoch: 36 [30000/60000 (50%)]\tLoss: 142.208486\n",
      "INFO:root:====> Epoch: 36 Average training loss: 143.2882\n",
      "INFO:root:====> Epoch: 36 Average testing loss: 143.6280\n",
      "INFO:root:Train Epoch: 37 [0/60000 (0%)]\tLoss: 140.677949\n",
      "INFO:root:Train Epoch: 37 [30000/60000 (50%)]\tLoss: 143.102520\n",
      "INFO:root:====> Epoch: 37 Average training loss: 143.1017\n",
      "INFO:root:====> Epoch: 37 Average testing loss: 143.7865\n",
      "INFO:root:Train Epoch: 38 [0/60000 (0%)]\tLoss: 150.818125\n",
      "INFO:root:Train Epoch: 38 [30000/60000 (50%)]\tLoss: 144.768291\n",
      "INFO:root:====> Epoch: 38 Average training loss: 142.9665\n",
      "INFO:root:====> Epoch: 38 Average testing loss: 143.1135\n",
      "INFO:root:Train Epoch: 39 [0/60000 (0%)]\tLoss: 133.988301\n",
      "INFO:root:Train Epoch: 39 [30000/60000 (50%)]\tLoss: 141.286875\n",
      "INFO:root:====> Epoch: 39 Average training loss: 142.8041\n",
      "INFO:root:====> Epoch: 39 Average testing loss: 143.2474\n",
      "INFO:root:Train Epoch: 40 [0/60000 (0%)]\tLoss: 134.654443\n",
      "INFO:root:Train Epoch: 40 [30000/60000 (50%)]\tLoss: 138.256572\n",
      "INFO:root:====> Epoch: 40 Average training loss: 142.6786\n",
      "INFO:root:====> Epoch: 40 Average testing loss: 142.8227\n",
      "INFO:root:Train Epoch: 41 [0/60000 (0%)]\tLoss: 143.994082\n",
      "INFO:root:Train Epoch: 41 [30000/60000 (50%)]\tLoss: 141.366396\n",
      "INFO:root:====> Epoch: 41 Average training loss: 142.5577\n",
      "INFO:root:====> Epoch: 41 Average testing loss: 142.7494\n",
      "INFO:root:Train Epoch: 42 [0/60000 (0%)]\tLoss: 142.186152\n",
      "INFO:root:Train Epoch: 42 [30000/60000 (50%)]\tLoss: 146.745156\n",
      "INFO:root:====> Epoch: 42 Average training loss: 142.3935\n",
      "INFO:root:====> Epoch: 42 Average testing loss: 143.0734\n",
      "INFO:root:Train Epoch: 43 [0/60000 (0%)]\tLoss: 141.366904\n",
      "INFO:root:Train Epoch: 43 [30000/60000 (50%)]\tLoss: 139.558496\n",
      "INFO:root:====> Epoch: 43 Average training loss: 142.2796\n",
      "INFO:root:====> Epoch: 43 Average testing loss: 142.5089\n",
      "INFO:root:Train Epoch: 44 [0/60000 (0%)]\tLoss: 143.817412\n",
      "INFO:root:Train Epoch: 44 [30000/60000 (50%)]\tLoss: 140.120049\n",
      "INFO:root:====> Epoch: 44 Average training loss: 142.1332\n",
      "INFO:root:====> Epoch: 44 Average testing loss: 142.4553\n",
      "INFO:root:Train Epoch: 45 [0/60000 (0%)]\tLoss: 145.112783\n",
      "INFO:root:Train Epoch: 45 [30000/60000 (50%)]\tLoss: 139.023447\n",
      "INFO:root:====> Epoch: 45 Average training loss: 142.0100\n",
      "INFO:root:====> Epoch: 45 Average testing loss: 142.4828\n",
      "INFO:root:Train Epoch: 46 [0/60000 (0%)]\tLoss: 141.620762\n",
      "INFO:root:Train Epoch: 46 [30000/60000 (50%)]\tLoss: 136.786982\n",
      "INFO:root:====> Epoch: 46 Average training loss: 141.9127\n",
      "INFO:root:====> Epoch: 46 Average testing loss: 142.2147\n",
      "INFO:root:Train Epoch: 47 [0/60000 (0%)]\tLoss: 142.204346\n",
      "INFO:root:Train Epoch: 47 [30000/60000 (50%)]\tLoss: 144.062754\n",
      "INFO:root:====> Epoch: 47 Average training loss: 141.7581\n",
      "INFO:root:====> Epoch: 47 Average testing loss: 142.1662\n",
      "INFO:root:Train Epoch: 48 [0/60000 (0%)]\tLoss: 147.936992\n",
      "INFO:root:Train Epoch: 48 [30000/60000 (50%)]\tLoss: 136.027148\n",
      "INFO:root:====> Epoch: 48 Average training loss: 141.6620\n",
      "INFO:root:====> Epoch: 48 Average testing loss: 142.5525\n",
      "INFO:root:Train Epoch: 49 [0/60000 (0%)]\tLoss: 145.975537\n",
      "INFO:root:Train Epoch: 49 [30000/60000 (50%)]\tLoss: 139.828564\n",
      "INFO:root:====> Epoch: 49 Average training loss: 141.5278\n",
      "INFO:root:====> Epoch: 49 Average testing loss: 141.9800\n",
      "INFO:root:Train Epoch: 50 [0/60000 (0%)]\tLoss: 143.902900\n",
      "INFO:root:Train Epoch: 50 [30000/60000 (50%)]\tLoss: 142.328770\n",
      "INFO:root:====> Epoch: 50 Average training loss: 141.4437\n",
      "INFO:root:====> Epoch: 50 Average testing loss: 141.8337\n",
      "INFO:root:Train Epoch: 51 [0/60000 (0%)]\tLoss: 141.979141\n",
      "INFO:root:Train Epoch: 51 [30000/60000 (50%)]\tLoss: 144.651113\n",
      "INFO:root:====> Epoch: 51 Average training loss: 141.3251\n",
      "INFO:root:====> Epoch: 51 Average testing loss: 141.7009\n",
      "INFO:root:Train Epoch: 52 [0/60000 (0%)]\tLoss: 145.259658\n",
      "INFO:root:Train Epoch: 52 [30000/60000 (50%)]\tLoss: 141.678594\n",
      "INFO:root:====> Epoch: 52 Average training loss: 141.2353\n",
      "INFO:root:====> Epoch: 52 Average testing loss: 141.6881\n",
      "INFO:root:Train Epoch: 53 [0/60000 (0%)]\tLoss: 141.780352\n",
      "INFO:root:Train Epoch: 53 [30000/60000 (50%)]\tLoss: 141.628926\n",
      "INFO:root:====> Epoch: 53 Average training loss: 141.1126\n",
      "INFO:root:====> Epoch: 53 Average testing loss: 141.7672\n",
      "INFO:root:Train Epoch: 54 [0/60000 (0%)]\tLoss: 140.702441\n",
      "INFO:root:Train Epoch: 54 [30000/60000 (50%)]\tLoss: 141.907305\n",
      "INFO:root:====> Epoch: 54 Average training loss: 141.0161\n",
      "INFO:root:====> Epoch: 54 Average testing loss: 141.6298\n",
      "INFO:root:Train Epoch: 55 [0/60000 (0%)]\tLoss: 140.015000\n",
      "INFO:root:Train Epoch: 55 [30000/60000 (50%)]\tLoss: 132.813105\n",
      "INFO:root:====> Epoch: 55 Average training loss: 140.9244\n",
      "INFO:root:====> Epoch: 55 Average testing loss: 141.3705\n",
      "INFO:root:Train Epoch: 56 [0/60000 (0%)]\tLoss: 133.991270\n",
      "INFO:root:Train Epoch: 56 [30000/60000 (50%)]\tLoss: 137.565566\n",
      "INFO:root:====> Epoch: 56 Average training loss: 140.8198\n",
      "INFO:root:====> Epoch: 56 Average testing loss: 141.3557\n",
      "INFO:root:Train Epoch: 57 [0/60000 (0%)]\tLoss: 137.593467\n",
      "INFO:root:Train Epoch: 57 [30000/60000 (50%)]\tLoss: 136.270400\n",
      "INFO:root:====> Epoch: 57 Average training loss: 140.7346\n",
      "INFO:root:====> Epoch: 57 Average testing loss: 141.3189\n",
      "INFO:root:Train Epoch: 58 [0/60000 (0%)]\tLoss: 143.607969\n",
      "INFO:root:Train Epoch: 58 [30000/60000 (50%)]\tLoss: 140.225166\n",
      "INFO:root:====> Epoch: 58 Average training loss: 140.6361\n",
      "INFO:root:====> Epoch: 58 Average testing loss: 141.1655\n",
      "INFO:root:Train Epoch: 59 [0/60000 (0%)]\tLoss: 140.475352\n",
      "INFO:root:Train Epoch: 59 [30000/60000 (50%)]\tLoss: 136.636963\n",
      "INFO:root:====> Epoch: 59 Average training loss: 140.5401\n",
      "INFO:root:====> Epoch: 59 Average testing loss: 141.2278\n",
      "INFO:root:Train Epoch: 60 [0/60000 (0%)]\tLoss: 137.616084\n",
      "INFO:root:Train Epoch: 60 [30000/60000 (50%)]\tLoss: 139.975332\n",
      "INFO:root:====> Epoch: 60 Average training loss: 140.4392\n",
      "INFO:root:====> Epoch: 60 Average testing loss: 141.1288\n",
      "INFO:root:Train Epoch: 61 [0/60000 (0%)]\tLoss: 146.035137\n",
      "INFO:root:Train Epoch: 61 [30000/60000 (50%)]\tLoss: 143.637324\n",
      "INFO:root:====> Epoch: 61 Average training loss: 140.3495\n",
      "INFO:root:====> Epoch: 61 Average testing loss: 141.1605\n",
      "INFO:root:Train Epoch: 62 [0/60000 (0%)]\tLoss: 140.851025\n",
      "INFO:root:Train Epoch: 62 [30000/60000 (50%)]\tLoss: 130.107256\n",
      "INFO:root:====> Epoch: 62 Average training loss: 140.2611\n",
      "INFO:root:====> Epoch: 62 Average testing loss: 140.9678\n",
      "INFO:root:Train Epoch: 63 [0/60000 (0%)]\tLoss: 139.199473\n",
      "INFO:root:Train Epoch: 63 [30000/60000 (50%)]\tLoss: 142.785322\n",
      "INFO:root:====> Epoch: 63 Average training loss: 140.1676\n",
      "INFO:root:====> Epoch: 63 Average testing loss: 140.9977\n",
      "INFO:root:Train Epoch: 64 [0/60000 (0%)]\tLoss: 149.371895\n",
      "INFO:root:Train Epoch: 64 [30000/60000 (50%)]\tLoss: 144.547881\n",
      "INFO:root:====> Epoch: 64 Average training loss: 140.0958\n",
      "INFO:root:====> Epoch: 64 Average testing loss: 140.7126\n",
      "INFO:root:Train Epoch: 65 [0/60000 (0%)]\tLoss: 133.746143\n",
      "INFO:root:Train Epoch: 65 [30000/60000 (50%)]\tLoss: 141.863652\n",
      "INFO:root:====> Epoch: 65 Average training loss: 140.0241\n",
      "INFO:root:====> Epoch: 65 Average testing loss: 140.8233\n",
      "INFO:root:Train Epoch: 66 [0/60000 (0%)]\tLoss: 142.168584\n",
      "INFO:root:Train Epoch: 66 [30000/60000 (50%)]\tLoss: 143.726240\n",
      "INFO:root:====> Epoch: 66 Average training loss: 139.9453\n",
      "INFO:root:====> Epoch: 66 Average testing loss: 140.8818\n",
      "INFO:root:Train Epoch: 67 [0/60000 (0%)]\tLoss: 144.470664\n",
      "INFO:root:Train Epoch: 67 [30000/60000 (50%)]\tLoss: 125.668076\n",
      "INFO:root:====> Epoch: 67 Average training loss: 139.8372\n",
      "INFO:root:====> Epoch: 67 Average testing loss: 140.7810\n",
      "INFO:root:Train Epoch: 68 [0/60000 (0%)]\tLoss: 141.550010\n",
      "INFO:root:Train Epoch: 68 [30000/60000 (50%)]\tLoss: 130.259990\n",
      "INFO:root:====> Epoch: 68 Average training loss: 139.7887\n",
      "INFO:root:====> Epoch: 68 Average testing loss: 140.6689\n",
      "INFO:root:Train Epoch: 69 [0/60000 (0%)]\tLoss: 139.122422\n",
      "INFO:root:Train Epoch: 69 [30000/60000 (50%)]\tLoss: 137.962900\n",
      "INFO:root:====> Epoch: 69 Average training loss: 139.6992\n",
      "INFO:root:====> Epoch: 69 Average testing loss: 140.4699\n",
      "INFO:root:Train Epoch: 70 [0/60000 (0%)]\tLoss: 148.457900\n",
      "INFO:root:Train Epoch: 70 [30000/60000 (50%)]\tLoss: 144.322842\n",
      "INFO:root:====> Epoch: 70 Average training loss: 139.6450\n",
      "INFO:root:====> Epoch: 70 Average testing loss: 140.4888\n",
      "INFO:root:Train Epoch: 71 [0/60000 (0%)]\tLoss: 139.353770\n",
      "INFO:root:Train Epoch: 71 [30000/60000 (50%)]\tLoss: 137.989248\n",
      "INFO:root:====> Epoch: 71 Average training loss: 139.5684\n",
      "INFO:root:====> Epoch: 71 Average testing loss: 140.3218\n",
      "INFO:root:Train Epoch: 72 [0/60000 (0%)]\tLoss: 135.335537\n",
      "INFO:root:Train Epoch: 72 [30000/60000 (50%)]\tLoss: 142.312979\n",
      "INFO:root:====> Epoch: 72 Average training loss: 139.4614\n",
      "INFO:root:====> Epoch: 72 Average testing loss: 140.2684\n",
      "INFO:root:Train Epoch: 73 [0/60000 (0%)]\tLoss: 139.183838\n",
      "INFO:root:Train Epoch: 73 [30000/60000 (50%)]\tLoss: 133.343584\n",
      "INFO:root:====> Epoch: 73 Average training loss: 139.4073\n",
      "INFO:root:====> Epoch: 73 Average testing loss: 140.2051\n",
      "INFO:root:Train Epoch: 74 [0/60000 (0%)]\tLoss: 149.370020\n",
      "INFO:root:Train Epoch: 74 [30000/60000 (50%)]\tLoss: 136.151953\n",
      "INFO:root:====> Epoch: 74 Average training loss: 139.3411\n",
      "INFO:root:====> Epoch: 74 Average testing loss: 140.1631\n",
      "INFO:root:Train Epoch: 75 [0/60000 (0%)]\tLoss: 136.224180\n",
      "INFO:root:Train Epoch: 75 [30000/60000 (50%)]\tLoss: 136.687344\n",
      "INFO:root:====> Epoch: 75 Average training loss: 139.2449\n",
      "INFO:root:====> Epoch: 75 Average testing loss: 140.0648\n",
      "INFO:root:Train Epoch: 76 [0/60000 (0%)]\tLoss: 139.990869\n",
      "INFO:root:Train Epoch: 76 [30000/60000 (50%)]\tLoss: 132.133018\n",
      "INFO:root:====> Epoch: 76 Average training loss: 139.1661\n",
      "INFO:root:====> Epoch: 76 Average testing loss: 140.0794\n",
      "INFO:root:Train Epoch: 77 [0/60000 (0%)]\tLoss: 140.776104\n",
      "INFO:root:Train Epoch: 77 [30000/60000 (50%)]\tLoss: 135.185918\n",
      "INFO:root:====> Epoch: 77 Average training loss: 139.1158\n",
      "INFO:root:====> Epoch: 77 Average testing loss: 140.1115\n",
      "INFO:root:Train Epoch: 78 [0/60000 (0%)]\tLoss: 140.784160\n",
      "INFO:root:Train Epoch: 78 [30000/60000 (50%)]\tLoss: 140.682217\n",
      "INFO:root:====> Epoch: 78 Average training loss: 139.0671\n",
      "INFO:root:====> Epoch: 78 Average testing loss: 140.0693\n",
      "INFO:root:Train Epoch: 79 [0/60000 (0%)]\tLoss: 134.626738\n",
      "INFO:root:Train Epoch: 79 [30000/60000 (50%)]\tLoss: 138.037939\n",
      "INFO:root:====> Epoch: 79 Average training loss: 138.9844\n",
      "INFO:root:====> Epoch: 79 Average testing loss: 139.9137\n",
      "INFO:root:Train Epoch: 80 [0/60000 (0%)]\tLoss: 134.401074\n",
      "INFO:root:Train Epoch: 80 [30000/60000 (50%)]\tLoss: 141.210879\n",
      "INFO:root:====> Epoch: 80 Average training loss: 138.9314\n",
      "INFO:root:====> Epoch: 80 Average testing loss: 139.8249\n",
      "INFO:root:Train Epoch: 81 [0/60000 (0%)]\tLoss: 134.938779\n",
      "INFO:root:Train Epoch: 81 [30000/60000 (50%)]\tLoss: 151.773750\n",
      "INFO:root:====> Epoch: 81 Average training loss: 138.8622\n",
      "INFO:root:====> Epoch: 81 Average testing loss: 139.9239\n",
      "INFO:root:Train Epoch: 82 [0/60000 (0%)]\tLoss: 138.077891\n",
      "INFO:root:Train Epoch: 82 [30000/60000 (50%)]\tLoss: 135.130830\n",
      "INFO:root:====> Epoch: 82 Average training loss: 138.7979\n",
      "INFO:root:====> Epoch: 82 Average testing loss: 139.9933\n",
      "INFO:root:Train Epoch: 83 [0/60000 (0%)]\tLoss: 130.586416\n",
      "INFO:root:Train Epoch: 83 [30000/60000 (50%)]\tLoss: 142.585615\n",
      "INFO:root:====> Epoch: 83 Average training loss: 138.7121\n",
      "INFO:root:====> Epoch: 83 Average testing loss: 139.6722\n",
      "INFO:root:Train Epoch: 84 [0/60000 (0%)]\tLoss: 135.885967\n",
      "INFO:root:Train Epoch: 84 [30000/60000 (50%)]\tLoss: 133.349805\n",
      "INFO:root:====> Epoch: 84 Average training loss: 138.6571\n",
      "INFO:root:====> Epoch: 84 Average testing loss: 139.7592\n",
      "INFO:root:Train Epoch: 85 [0/60000 (0%)]\tLoss: 137.112119\n",
      "INFO:root:Train Epoch: 85 [30000/60000 (50%)]\tLoss: 136.594131\n",
      "INFO:root:====> Epoch: 85 Average training loss: 138.5851\n",
      "INFO:root:====> Epoch: 85 Average testing loss: 139.5685\n",
      "INFO:root:Train Epoch: 86 [0/60000 (0%)]\tLoss: 135.347246\n",
      "INFO:root:Train Epoch: 86 [30000/60000 (50%)]\tLoss: 136.997500\n",
      "INFO:root:====> Epoch: 86 Average training loss: 138.5385\n",
      "INFO:root:====> Epoch: 86 Average testing loss: 139.6604\n",
      "INFO:root:Train Epoch: 87 [0/60000 (0%)]\tLoss: 143.677598\n",
      "INFO:root:Train Epoch: 87 [30000/60000 (50%)]\tLoss: 140.263535\n",
      "INFO:root:====> Epoch: 87 Average training loss: 138.4850\n",
      "INFO:root:====> Epoch: 87 Average testing loss: 139.6785\n",
      "INFO:root:Train Epoch: 88 [0/60000 (0%)]\tLoss: 133.763818\n",
      "INFO:root:Train Epoch: 88 [30000/60000 (50%)]\tLoss: 145.534121\n",
      "INFO:root:====> Epoch: 88 Average training loss: 138.3920\n",
      "INFO:root:====> Epoch: 88 Average testing loss: 139.4195\n",
      "INFO:root:Train Epoch: 89 [0/60000 (0%)]\tLoss: 129.169082\n",
      "INFO:root:Train Epoch: 89 [30000/60000 (50%)]\tLoss: 138.529023\n",
      "INFO:root:====> Epoch: 89 Average training loss: 138.3551\n",
      "INFO:root:====> Epoch: 89 Average testing loss: 139.5505\n",
      "INFO:root:Train Epoch: 90 [0/60000 (0%)]\tLoss: 133.900557\n",
      "INFO:root:Train Epoch: 90 [30000/60000 (50%)]\tLoss: 139.011582\n",
      "INFO:root:====> Epoch: 90 Average training loss: 138.3009\n",
      "INFO:root:====> Epoch: 90 Average testing loss: 139.4097\n",
      "INFO:root:Train Epoch: 91 [0/60000 (0%)]\tLoss: 137.647549\n",
      "INFO:root:Train Epoch: 91 [30000/60000 (50%)]\tLoss: 136.616875\n",
      "INFO:root:====> Epoch: 91 Average training loss: 138.2398\n",
      "INFO:root:====> Epoch: 91 Average testing loss: 139.3302\n",
      "INFO:root:Train Epoch: 92 [0/60000 (0%)]\tLoss: 130.518066\n",
      "INFO:root:Train Epoch: 92 [30000/60000 (50%)]\tLoss: 130.380254\n",
      "INFO:root:====> Epoch: 92 Average training loss: 138.1790\n",
      "INFO:root:====> Epoch: 92 Average testing loss: 139.3186\n",
      "INFO:root:Train Epoch: 93 [0/60000 (0%)]\tLoss: 137.035381\n",
      "INFO:root:Train Epoch: 93 [30000/60000 (50%)]\tLoss: 140.215332\n",
      "INFO:root:====> Epoch: 93 Average training loss: 138.1109\n",
      "INFO:root:====> Epoch: 93 Average testing loss: 139.2976\n",
      "INFO:root:Train Epoch: 94 [0/60000 (0%)]\tLoss: 143.623965\n",
      "INFO:root:Train Epoch: 94 [30000/60000 (50%)]\tLoss: 137.347295\n",
      "INFO:root:====> Epoch: 94 Average training loss: 138.0775\n",
      "INFO:root:====> Epoch: 94 Average testing loss: 139.1841\n",
      "INFO:root:Train Epoch: 95 [0/60000 (0%)]\tLoss: 127.758301\n",
      "INFO:root:Train Epoch: 95 [30000/60000 (50%)]\tLoss: 142.894023\n",
      "INFO:root:====> Epoch: 95 Average training loss: 138.0110\n",
      "INFO:root:====> Epoch: 95 Average testing loss: 139.1403\n",
      "INFO:root:Train Epoch: 96 [0/60000 (0%)]\tLoss: 142.266494\n",
      "INFO:root:Train Epoch: 96 [30000/60000 (50%)]\tLoss: 147.038730\n",
      "INFO:root:====> Epoch: 96 Average training loss: 137.9634\n",
      "INFO:root:====> Epoch: 96 Average testing loss: 139.1037\n",
      "INFO:root:Train Epoch: 97 [0/60000 (0%)]\tLoss: 145.027646\n",
      "INFO:root:Train Epoch: 97 [30000/60000 (50%)]\tLoss: 145.782666\n",
      "INFO:root:====> Epoch: 97 Average training loss: 137.8949\n",
      "INFO:root:====> Epoch: 97 Average testing loss: 139.2842\n",
      "INFO:root:Train Epoch: 98 [0/60000 (0%)]\tLoss: 142.078760\n",
      "INFO:root:Train Epoch: 98 [30000/60000 (50%)]\tLoss: 139.861309\n",
      "INFO:root:====> Epoch: 98 Average training loss: 137.8356\n",
      "INFO:root:====> Epoch: 98 Average testing loss: 139.0728\n",
      "INFO:root:Train Epoch: 99 [0/60000 (0%)]\tLoss: 140.988301\n",
      "INFO:root:Train Epoch: 99 [30000/60000 (50%)]\tLoss: 141.088320\n",
      "INFO:root:====> Epoch: 99 Average training loss: 137.7986\n",
      "INFO:root:====> Epoch: 99 Average testing loss: 139.0118\n",
      "INFO:root:Train Epoch: 100 [0/60000 (0%)]\tLoss: 139.753574\n",
      "INFO:root:Train Epoch: 100 [30000/60000 (50%)]\tLoss: 139.543896\n",
      "INFO:root:====> Epoch: 100 Average training loss: 137.7389\n",
      "INFO:root:====> Epoch: 100 Average testing loss: 138.9777\n",
      "INFO:root:Train Epoch: 101 [0/60000 (0%)]\tLoss: 137.019951\n",
      "INFO:root:Train Epoch: 101 [30000/60000 (50%)]\tLoss: 133.119531\n",
      "INFO:root:====> Epoch: 101 Average training loss: 137.6909\n",
      "INFO:root:====> Epoch: 101 Average testing loss: 138.8838\n",
      "INFO:root:Train Epoch: 102 [0/60000 (0%)]\tLoss: 142.918652\n",
      "INFO:root:Train Epoch: 102 [30000/60000 (50%)]\tLoss: 139.795752\n",
      "INFO:root:====> Epoch: 102 Average training loss: 137.6242\n",
      "INFO:root:====> Epoch: 102 Average testing loss: 138.8588\n",
      "INFO:root:Train Epoch: 103 [0/60000 (0%)]\tLoss: 137.469014\n",
      "INFO:root:Train Epoch: 103 [30000/60000 (50%)]\tLoss: 135.420977\n",
      "INFO:root:====> Epoch: 103 Average training loss: 137.5838\n",
      "INFO:root:====> Epoch: 103 Average testing loss: 138.8418\n",
      "INFO:root:Train Epoch: 104 [0/60000 (0%)]\tLoss: 138.043135\n",
      "INFO:root:Train Epoch: 104 [30000/60000 (50%)]\tLoss: 139.955264\n",
      "INFO:root:====> Epoch: 104 Average training loss: 137.5584\n",
      "INFO:root:====> Epoch: 104 Average testing loss: 138.9757\n",
      "INFO:root:Train Epoch: 105 [0/60000 (0%)]\tLoss: 136.008506\n",
      "INFO:root:Train Epoch: 105 [30000/60000 (50%)]\tLoss: 141.361533\n",
      "INFO:root:====> Epoch: 105 Average training loss: 137.5038\n",
      "INFO:root:====> Epoch: 105 Average testing loss: 138.7757\n",
      "INFO:root:Train Epoch: 106 [0/60000 (0%)]\tLoss: 134.367637\n",
      "INFO:root:Train Epoch: 106 [30000/60000 (50%)]\tLoss: 133.661875\n",
      "INFO:root:====> Epoch: 106 Average training loss: 137.4537\n",
      "INFO:root:====> Epoch: 106 Average testing loss: 138.8861\n",
      "INFO:root:Train Epoch: 107 [0/60000 (0%)]\tLoss: 141.669521\n",
      "INFO:root:Train Epoch: 107 [30000/60000 (50%)]\tLoss: 135.491006\n",
      "INFO:root:====> Epoch: 107 Average training loss: 137.4043\n",
      "INFO:root:====> Epoch: 107 Average testing loss: 138.7514\n",
      "INFO:root:Train Epoch: 108 [0/60000 (0%)]\tLoss: 129.699375\n",
      "INFO:root:Train Epoch: 108 [30000/60000 (50%)]\tLoss: 137.618896\n",
      "INFO:root:====> Epoch: 108 Average training loss: 137.3433\n",
      "INFO:root:====> Epoch: 108 Average testing loss: 138.6878\n",
      "INFO:root:Train Epoch: 109 [0/60000 (0%)]\tLoss: 133.477598\n",
      "INFO:root:Train Epoch: 109 [30000/60000 (50%)]\tLoss: 135.181670\n",
      "INFO:root:====> Epoch: 109 Average training loss: 137.2970\n",
      "INFO:root:====> Epoch: 109 Average testing loss: 138.6343\n",
      "INFO:root:Train Epoch: 110 [0/60000 (0%)]\tLoss: 140.225645\n",
      "INFO:root:Train Epoch: 110 [30000/60000 (50%)]\tLoss: 138.047842\n",
      "INFO:root:====> Epoch: 110 Average training loss: 137.2611\n",
      "INFO:root:====> Epoch: 110 Average testing loss: 138.6367\n",
      "INFO:root:Train Epoch: 111 [0/60000 (0%)]\tLoss: 143.398555\n",
      "INFO:root:Train Epoch: 111 [30000/60000 (50%)]\tLoss: 133.166279\n",
      "INFO:root:====> Epoch: 111 Average training loss: 137.2168\n",
      "INFO:root:====> Epoch: 111 Average testing loss: 138.5544\n",
      "INFO:root:Train Epoch: 112 [0/60000 (0%)]\tLoss: 140.093447\n",
      "INFO:root:Train Epoch: 112 [30000/60000 (50%)]\tLoss: 129.906025\n",
      "INFO:root:====> Epoch: 112 Average training loss: 137.1699\n",
      "INFO:root:====> Epoch: 112 Average testing loss: 138.6138\n",
      "INFO:root:Train Epoch: 113 [0/60000 (0%)]\tLoss: 130.818662\n",
      "INFO:root:Train Epoch: 113 [30000/60000 (50%)]\tLoss: 142.798770\n",
      "INFO:root:====> Epoch: 113 Average training loss: 137.1059\n",
      "INFO:root:====> Epoch: 113 Average testing loss: 138.5184\n",
      "INFO:root:Train Epoch: 114 [0/60000 (0%)]\tLoss: 137.529639\n",
      "INFO:root:Train Epoch: 114 [30000/60000 (50%)]\tLoss: 135.167412\n",
      "INFO:root:====> Epoch: 114 Average training loss: 137.0736\n",
      "INFO:root:====> Epoch: 114 Average testing loss: 138.5563\n",
      "INFO:root:Train Epoch: 115 [0/60000 (0%)]\tLoss: 138.775020\n",
      "INFO:root:Train Epoch: 115 [30000/60000 (50%)]\tLoss: 134.932187\n",
      "INFO:root:====> Epoch: 115 Average training loss: 137.0054\n",
      "INFO:root:====> Epoch: 115 Average testing loss: 138.4665\n",
      "INFO:root:Train Epoch: 116 [0/60000 (0%)]\tLoss: 140.518311\n",
      "INFO:root:Train Epoch: 116 [30000/60000 (50%)]\tLoss: 126.390400\n",
      "INFO:root:====> Epoch: 116 Average training loss: 136.9717\n",
      "INFO:root:====> Epoch: 116 Average testing loss: 138.6644\n",
      "INFO:root:Train Epoch: 117 [0/60000 (0%)]\tLoss: 132.478125\n",
      "INFO:root:Train Epoch: 117 [30000/60000 (50%)]\tLoss: 138.425049\n",
      "INFO:root:====> Epoch: 117 Average training loss: 136.9119\n",
      "INFO:root:====> Epoch: 117 Average testing loss: 138.3865\n",
      "INFO:root:Train Epoch: 118 [0/60000 (0%)]\tLoss: 143.054531\n",
      "INFO:root:Train Epoch: 118 [30000/60000 (50%)]\tLoss: 136.792188\n",
      "INFO:root:====> Epoch: 118 Average training loss: 136.8781\n",
      "INFO:root:====> Epoch: 118 Average testing loss: 138.4809\n",
      "INFO:root:Train Epoch: 119 [0/60000 (0%)]\tLoss: 147.745527\n",
      "INFO:root:Train Epoch: 119 [30000/60000 (50%)]\tLoss: 138.180977\n",
      "INFO:root:====> Epoch: 119 Average training loss: 136.8564\n",
      "INFO:root:====> Epoch: 119 Average testing loss: 138.2967\n",
      "INFO:root:Train Epoch: 120 [0/60000 (0%)]\tLoss: 141.566250\n",
      "INFO:root:Train Epoch: 120 [30000/60000 (50%)]\tLoss: 140.044629\n",
      "INFO:root:====> Epoch: 120 Average training loss: 136.8223\n",
      "INFO:root:====> Epoch: 120 Average testing loss: 138.2720\n",
      "INFO:root:Train Epoch: 121 [0/60000 (0%)]\tLoss: 136.651709\n",
      "INFO:root:Train Epoch: 121 [30000/60000 (50%)]\tLoss: 142.806064\n",
      "INFO:root:====> Epoch: 121 Average training loss: 136.7665\n",
      "INFO:root:====> Epoch: 121 Average testing loss: 138.3058\n",
      "INFO:root:Train Epoch: 122 [0/60000 (0%)]\tLoss: 131.742686\n",
      "INFO:root:Train Epoch: 122 [30000/60000 (50%)]\tLoss: 128.293115\n",
      "INFO:root:====> Epoch: 122 Average training loss: 136.7312\n",
      "INFO:root:====> Epoch: 122 Average testing loss: 138.1044\n",
      "INFO:root:Train Epoch: 123 [0/60000 (0%)]\tLoss: 131.131445\n",
      "INFO:root:Train Epoch: 123 [30000/60000 (50%)]\tLoss: 141.358555\n",
      "INFO:root:====> Epoch: 123 Average training loss: 136.7126\n",
      "INFO:root:====> Epoch: 123 Average testing loss: 138.2104\n",
      "INFO:root:Train Epoch: 124 [0/60000 (0%)]\tLoss: 145.031709\n",
      "INFO:root:Train Epoch: 124 [30000/60000 (50%)]\tLoss: 146.528770\n",
      "INFO:root:====> Epoch: 124 Average training loss: 136.6380\n",
      "INFO:root:====> Epoch: 124 Average testing loss: 138.2083\n",
      "INFO:root:Train Epoch: 125 [0/60000 (0%)]\tLoss: 128.661602\n",
      "INFO:root:Train Epoch: 125 [30000/60000 (50%)]\tLoss: 135.978252\n",
      "INFO:root:====> Epoch: 125 Average training loss: 136.5916\n",
      "INFO:root:====> Epoch: 125 Average testing loss: 138.1824\n",
      "INFO:root:Train Epoch: 126 [0/60000 (0%)]\tLoss: 128.736992\n",
      "INFO:root:Train Epoch: 126 [30000/60000 (50%)]\tLoss: 135.660234\n",
      "INFO:root:====> Epoch: 126 Average training loss: 136.5615\n",
      "INFO:root:====> Epoch: 126 Average testing loss: 138.2011\n",
      "INFO:root:Train Epoch: 127 [0/60000 (0%)]\tLoss: 136.537334\n",
      "INFO:root:Train Epoch: 127 [30000/60000 (50%)]\tLoss: 136.719072\n",
      "INFO:root:====> Epoch: 127 Average training loss: 136.5460\n",
      "INFO:root:====> Epoch: 127 Average testing loss: 138.1095\n",
      "INFO:root:Train Epoch: 128 [0/60000 (0%)]\tLoss: 135.272764\n",
      "INFO:root:Train Epoch: 128 [30000/60000 (50%)]\tLoss: 135.747168\n",
      "INFO:root:====> Epoch: 128 Average training loss: 136.4755\n",
      "INFO:root:====> Epoch: 128 Average testing loss: 138.0535\n",
      "INFO:root:Train Epoch: 129 [0/60000 (0%)]\tLoss: 136.626357\n",
      "INFO:root:Train Epoch: 129 [30000/60000 (50%)]\tLoss: 133.986855\n",
      "INFO:root:====> Epoch: 129 Average training loss: 136.4539\n",
      "INFO:root:====> Epoch: 129 Average testing loss: 138.0764\n",
      "INFO:root:Train Epoch: 130 [0/60000 (0%)]\tLoss: 133.965332\n",
      "INFO:root:Train Epoch: 130 [30000/60000 (50%)]\tLoss: 138.790625\n",
      "INFO:root:====> Epoch: 130 Average training loss: 136.3889\n",
      "INFO:root:====> Epoch: 130 Average testing loss: 138.0177\n",
      "INFO:root:Train Epoch: 131 [0/60000 (0%)]\tLoss: 137.197793\n",
      "INFO:root:Train Epoch: 131 [30000/60000 (50%)]\tLoss: 142.846113\n",
      "INFO:root:====> Epoch: 131 Average training loss: 136.3948\n",
      "INFO:root:====> Epoch: 131 Average testing loss: 138.0238\n",
      "INFO:root:Train Epoch: 132 [0/60000 (0%)]\tLoss: 139.489570\n",
      "INFO:root:Train Epoch: 132 [30000/60000 (50%)]\tLoss: 141.129316\n",
      "INFO:root:====> Epoch: 132 Average training loss: 136.3536\n",
      "INFO:root:====> Epoch: 132 Average testing loss: 138.0730\n",
      "INFO:root:Train Epoch: 133 [0/60000 (0%)]\tLoss: 140.911865\n",
      "INFO:root:Train Epoch: 133 [30000/60000 (50%)]\tLoss: 135.753223\n",
      "INFO:root:====> Epoch: 133 Average training loss: 136.3121\n",
      "INFO:root:====> Epoch: 133 Average testing loss: 138.0345\n",
      "INFO:root:Train Epoch: 134 [0/60000 (0%)]\tLoss: 140.281230\n",
      "INFO:root:Train Epoch: 134 [30000/60000 (50%)]\tLoss: 133.901914\n",
      "INFO:root:====> Epoch: 134 Average training loss: 136.2509\n",
      "INFO:root:====> Epoch: 134 Average testing loss: 137.9011\n",
      "INFO:root:Train Epoch: 135 [0/60000 (0%)]\tLoss: 138.044473\n",
      "INFO:root:Train Epoch: 135 [30000/60000 (50%)]\tLoss: 132.087607\n",
      "INFO:root:====> Epoch: 135 Average training loss: 136.2509\n",
      "INFO:root:====> Epoch: 135 Average testing loss: 137.9782\n",
      "INFO:root:Train Epoch: 136 [0/60000 (0%)]\tLoss: 140.860020\n",
      "INFO:root:Train Epoch: 136 [30000/60000 (50%)]\tLoss: 133.046367\n",
      "INFO:root:====> Epoch: 136 Average training loss: 136.1868\n",
      "INFO:root:====> Epoch: 136 Average testing loss: 137.8409\n",
      "INFO:root:Train Epoch: 137 [0/60000 (0%)]\tLoss: 133.499639\n",
      "INFO:root:Train Epoch: 137 [30000/60000 (50%)]\tLoss: 139.805557\n",
      "INFO:root:====> Epoch: 137 Average training loss: 136.1317\n",
      "INFO:root:====> Epoch: 137 Average testing loss: 137.9119\n",
      "INFO:root:Train Epoch: 138 [0/60000 (0%)]\tLoss: 134.174639\n",
      "INFO:root:Train Epoch: 138 [30000/60000 (50%)]\tLoss: 141.049023\n",
      "INFO:root:====> Epoch: 138 Average training loss: 136.1291\n",
      "INFO:root:====> Epoch: 138 Average testing loss: 137.8034\n",
      "INFO:root:Train Epoch: 139 [0/60000 (0%)]\tLoss: 131.856406\n",
      "INFO:root:Train Epoch: 139 [30000/60000 (50%)]\tLoss: 140.436914\n",
      "INFO:root:====> Epoch: 139 Average training loss: 136.0935\n",
      "INFO:root:====> Epoch: 139 Average testing loss: 137.9038\n",
      "INFO:root:Train Epoch: 140 [0/60000 (0%)]\tLoss: 140.591973\n",
      "INFO:root:Train Epoch: 140 [30000/60000 (50%)]\tLoss: 135.826289\n",
      "INFO:root:====> Epoch: 140 Average training loss: 136.0498\n",
      "INFO:root:====> Epoch: 140 Average testing loss: 137.8416\n",
      "INFO:root:Train Epoch: 141 [0/60000 (0%)]\tLoss: 130.429365\n",
      "INFO:root:Train Epoch: 141 [30000/60000 (50%)]\tLoss: 127.921025\n",
      "INFO:root:====> Epoch: 141 Average training loss: 136.0147\n",
      "INFO:root:====> Epoch: 141 Average testing loss: 137.7229\n",
      "INFO:root:Train Epoch: 142 [0/60000 (0%)]\tLoss: 132.307070\n",
      "INFO:root:Train Epoch: 142 [30000/60000 (50%)]\tLoss: 132.503232\n",
      "INFO:root:====> Epoch: 142 Average training loss: 135.9911\n",
      "INFO:root:====> Epoch: 142 Average testing loss: 137.7472\n",
      "INFO:root:Train Epoch: 143 [0/60000 (0%)]\tLoss: 141.715293\n",
      "INFO:root:Train Epoch: 143 [30000/60000 (50%)]\tLoss: 139.660049\n",
      "INFO:root:====> Epoch: 143 Average training loss: 135.9441\n",
      "INFO:root:====> Epoch: 143 Average testing loss: 137.7954\n",
      "INFO:root:Train Epoch: 144 [0/60000 (0%)]\tLoss: 131.167354\n",
      "INFO:root:Train Epoch: 144 [30000/60000 (50%)]\tLoss: 144.466338\n",
      "INFO:root:====> Epoch: 144 Average training loss: 135.9236\n",
      "INFO:root:====> Epoch: 144 Average testing loss: 137.8855\n",
      "INFO:root:Train Epoch: 145 [0/60000 (0%)]\tLoss: 135.348770\n",
      "INFO:root:Train Epoch: 145 [30000/60000 (50%)]\tLoss: 135.948281\n",
      "INFO:root:====> Epoch: 145 Average training loss: 135.9004\n",
      "INFO:root:====> Epoch: 145 Average testing loss: 137.7442\n",
      "INFO:root:Train Epoch: 146 [0/60000 (0%)]\tLoss: 131.550879\n",
      "INFO:root:Train Epoch: 146 [30000/60000 (50%)]\tLoss: 132.351826\n",
      "INFO:root:====> Epoch: 146 Average training loss: 135.8616\n",
      "INFO:root:====> Epoch: 146 Average testing loss: 137.7103\n",
      "INFO:root:Train Epoch: 147 [0/60000 (0%)]\tLoss: 135.289092\n",
      "INFO:root:Train Epoch: 147 [30000/60000 (50%)]\tLoss: 140.288379\n",
      "INFO:root:====> Epoch: 147 Average training loss: 135.8155\n",
      "INFO:root:====> Epoch: 147 Average testing loss: 137.6039\n",
      "INFO:root:Train Epoch: 148 [0/60000 (0%)]\tLoss: 133.339463\n",
      "INFO:root:Train Epoch: 148 [30000/60000 (50%)]\tLoss: 140.605430\n",
      "INFO:root:====> Epoch: 148 Average training loss: 135.7787\n",
      "INFO:root:====> Epoch: 148 Average testing loss: 137.7321\n",
      "INFO:root:Train Epoch: 149 [0/60000 (0%)]\tLoss: 130.384375\n",
      "INFO:root:Train Epoch: 149 [30000/60000 (50%)]\tLoss: 137.862500\n",
      "INFO:root:====> Epoch: 149 Average training loss: 135.7427\n",
      "INFO:root:====> Epoch: 149 Average testing loss: 137.6681\n",
      "INFO:root:Train Epoch: 150 [0/60000 (0%)]\tLoss: 140.050850\n",
      "INFO:root:Train Epoch: 150 [30000/60000 (50%)]\tLoss: 125.275010\n",
      "INFO:root:====> Epoch: 150 Average training loss: 135.7240\n",
      "INFO:root:====> Epoch: 150 Average testing loss: 137.6634\n",
      "INFO:root:Train Epoch: 151 [0/60000 (0%)]\tLoss: 132.378564\n",
      "INFO:root:Train Epoch: 151 [30000/60000 (50%)]\tLoss: 134.857754\n",
      "INFO:root:====> Epoch: 151 Average training loss: 135.6995\n",
      "INFO:root:====> Epoch: 151 Average testing loss: 137.6025\n",
      "INFO:root:Train Epoch: 152 [0/60000 (0%)]\tLoss: 133.347705\n",
      "INFO:root:Train Epoch: 152 [30000/60000 (50%)]\tLoss: 131.516924\n",
      "INFO:root:====> Epoch: 152 Average training loss: 135.6716\n",
      "INFO:root:====> Epoch: 152 Average testing loss: 137.6830\n",
      "INFO:root:Train Epoch: 153 [0/60000 (0%)]\tLoss: 138.793770\n",
      "INFO:root:Train Epoch: 153 [30000/60000 (50%)]\tLoss: 130.110156\n",
      "INFO:root:====> Epoch: 153 Average training loss: 135.6472\n",
      "INFO:root:====> Epoch: 153 Average testing loss: 137.6180\n",
      "INFO:root:Train Epoch: 154 [0/60000 (0%)]\tLoss: 131.683555\n",
      "INFO:root:Train Epoch: 154 [30000/60000 (50%)]\tLoss: 131.662734\n",
      "INFO:root:====> Epoch: 154 Average training loss: 135.6031\n",
      "INFO:root:====> Epoch: 154 Average testing loss: 137.5575\n",
      "INFO:root:Train Epoch: 155 [0/60000 (0%)]\tLoss: 142.694346\n",
      "INFO:root:Train Epoch: 155 [30000/60000 (50%)]\tLoss: 139.862246\n",
      "INFO:root:====> Epoch: 155 Average training loss: 135.5803\n",
      "INFO:root:====> Epoch: 155 Average testing loss: 137.5114\n",
      "INFO:root:Train Epoch: 156 [0/60000 (0%)]\tLoss: 128.752607\n",
      "INFO:root:Train Epoch: 156 [30000/60000 (50%)]\tLoss: 136.448857\n",
      "INFO:root:====> Epoch: 156 Average training loss: 135.5444\n",
      "INFO:root:====> Epoch: 156 Average testing loss: 137.5577\n",
      "INFO:root:Train Epoch: 157 [0/60000 (0%)]\tLoss: 137.040293\n",
      "INFO:root:Train Epoch: 157 [30000/60000 (50%)]\tLoss: 136.929746\n",
      "INFO:root:====> Epoch: 157 Average training loss: 135.5192\n",
      "INFO:root:====> Epoch: 157 Average testing loss: 137.4567\n",
      "INFO:root:Train Epoch: 158 [0/60000 (0%)]\tLoss: 140.207402\n",
      "INFO:root:Train Epoch: 158 [30000/60000 (50%)]\tLoss: 132.813711\n",
      "INFO:root:====> Epoch: 158 Average training loss: 135.4776\n",
      "INFO:root:====> Epoch: 158 Average testing loss: 137.7171\n",
      "INFO:root:Train Epoch: 159 [0/60000 (0%)]\tLoss: 135.482461\n",
      "INFO:root:Train Epoch: 159 [30000/60000 (50%)]\tLoss: 127.448027\n",
      "INFO:root:====> Epoch: 159 Average training loss: 135.4642\n",
      "INFO:root:====> Epoch: 159 Average testing loss: 137.4182\n",
      "INFO:root:Train Epoch: 160 [0/60000 (0%)]\tLoss: 128.342041\n",
      "INFO:root:Train Epoch: 160 [30000/60000 (50%)]\tLoss: 136.997598\n",
      "INFO:root:====> Epoch: 160 Average training loss: 135.4288\n",
      "INFO:root:====> Epoch: 160 Average testing loss: 137.4103\n",
      "INFO:root:Train Epoch: 161 [0/60000 (0%)]\tLoss: 146.518701\n",
      "INFO:root:Train Epoch: 161 [30000/60000 (50%)]\tLoss: 135.342363\n",
      "INFO:root:====> Epoch: 161 Average training loss: 135.4121\n",
      "INFO:root:====> Epoch: 161 Average testing loss: 137.3992\n",
      "INFO:root:Train Epoch: 162 [0/60000 (0%)]\tLoss: 141.662285\n",
      "INFO:root:Train Epoch: 162 [30000/60000 (50%)]\tLoss: 132.120137\n",
      "INFO:root:====> Epoch: 162 Average training loss: 135.3841\n",
      "INFO:root:====> Epoch: 162 Average testing loss: 137.5596\n",
      "INFO:root:Train Epoch: 163 [0/60000 (0%)]\tLoss: 136.150176\n",
      "INFO:root:Train Epoch: 163 [30000/60000 (50%)]\tLoss: 133.679033\n",
      "INFO:root:====> Epoch: 163 Average training loss: 135.3591\n",
      "INFO:root:====> Epoch: 163 Average testing loss: 137.3920\n",
      "INFO:root:Train Epoch: 164 [0/60000 (0%)]\tLoss: 134.350762\n",
      "INFO:root:Train Epoch: 164 [30000/60000 (50%)]\tLoss: 133.925342\n",
      "INFO:root:====> Epoch: 164 Average training loss: 135.3297\n",
      "INFO:root:====> Epoch: 164 Average testing loss: 137.4396\n",
      "INFO:root:Train Epoch: 165 [0/60000 (0%)]\tLoss: 132.332744\n",
      "INFO:root:Train Epoch: 165 [30000/60000 (50%)]\tLoss: 137.186270\n",
      "INFO:root:====> Epoch: 165 Average training loss: 135.2698\n",
      "INFO:root:====> Epoch: 165 Average testing loss: 137.3701\n",
      "INFO:root:Train Epoch: 166 [0/60000 (0%)]\tLoss: 128.008311\n",
      "INFO:root:Train Epoch: 166 [30000/60000 (50%)]\tLoss: 135.089980\n",
      "INFO:root:====> Epoch: 166 Average training loss: 135.2628\n",
      "INFO:root:====> Epoch: 166 Average testing loss: 137.3679\n",
      "INFO:root:Train Epoch: 167 [0/60000 (0%)]\tLoss: 141.576807\n",
      "INFO:root:Train Epoch: 167 [30000/60000 (50%)]\tLoss: 126.750645\n",
      "INFO:root:====> Epoch: 167 Average training loss: 135.2306\n",
      "INFO:root:====> Epoch: 167 Average testing loss: 137.3468\n",
      "INFO:root:Train Epoch: 168 [0/60000 (0%)]\tLoss: 138.398145\n",
      "INFO:root:Train Epoch: 168 [30000/60000 (50%)]\tLoss: 135.114404\n",
      "INFO:root:====> Epoch: 168 Average training loss: 135.2216\n",
      "INFO:root:====> Epoch: 168 Average testing loss: 137.3276\n",
      "INFO:root:Train Epoch: 169 [0/60000 (0%)]\tLoss: 137.895566\n",
      "INFO:root:Train Epoch: 169 [30000/60000 (50%)]\tLoss: 131.774297\n",
      "INFO:root:====> Epoch: 169 Average training loss: 135.1779\n",
      "INFO:root:====> Epoch: 169 Average testing loss: 137.3020\n",
      "INFO:root:Train Epoch: 170 [0/60000 (0%)]\tLoss: 133.292441\n",
      "INFO:root:Train Epoch: 170 [30000/60000 (50%)]\tLoss: 137.463174\n"
     ]
    }
   ]
  }
 ]
}
