{
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOG8Mlr4npAPaj/Lo8h5xjL"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install torch torchvision torchaudio matplotlib numpy"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MdsZXP1gUk8k",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543773047,
     "user_tz": -60,
     "elapsed": 3366,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "62d2b294-aeab-43aa-f3fb-82537f1a1fca",
    "execution": {
     "iopub.status.busy": "2023-01-01T08:23:54.608902Z",
     "iopub.execute_input": "2023-01-01T08:23:54.609469Z",
     "iopub.status.idle": "2023-01-01T08:24:04.062408Z",
     "shell.execute_reply.started": "2023-01-01T08:23:54.609418Z",
     "shell.execute_reply": "2023-01-01T08:24:04.061302Z"
    },
    "trusted": true
   },
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.11.0)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.12.0)\nRequirement already satisfied: torchaudio in /opt/conda/lib/python3.7/site-packages (0.11.0)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (3.5.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (1.21.6)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.1.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.1.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (21.3)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (4.33.3)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.4.3)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (2.8.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.9.24)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!ls\n",
    "!pwd"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-01T08:24:04.065109Z",
     "iopub.execute_input": "2023-01-01T08:24:04.065592Z",
     "iopub.status.idle": "2023-01-01T08:24:06.287746Z",
     "shell.execute_reply.started": "2023-01-01T08:24:04.065544Z",
     "shell.execute_reply": "2023-01-01T08:24:06.286431Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "text": "__notebook_source__.ipynb  data_mnist\t       logging_info\ncheckpoints\t\t   figures_experiment  state.db\n/kaggle/working\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import utils\n",
    "from time import time\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random as random\n",
    "import os\n",
    "from vaeee import VAE\n",
    "\n",
    "from plot_figures import *"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-01-01T08:24:06.310879Z",
     "iopub.execute_input": "2023-01-01T08:24:06.313524Z",
     "iopub.status.idle": "2023-01-01T08:24:06.321273Z",
     "shell.execute_reply.started": "2023-01-01T08:24:06.313476Z",
     "shell.execute_reply": "2023-01-01T08:24:06.320286Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# only needs to do this once, comment this out when it's done. It unzips the project into your google drive folder named dd2434-vae\n",
    "# !unzip \"/content/drive/My Drive/dd2434-vae/main.zip\" -d \"/content/drive/My Drive/dd2434-vae\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TYa9ItHxOcde",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672525159640,
     "user_tz": -60,
     "elapsed": 2112,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "2121ded9-d694-475a-91bf-a68ae3d1db08"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sys.platform"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "yAvnF2SmViF7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543812960,
     "user_tz": -60,
     "elapsed": 4,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "491b91e5-7d99-4bc1-828b-f651b69fdf60",
    "execution": {
     "iopub.status.busy": "2023-01-01T07:42:55.991934Z",
     "iopub.execute_input": "2023-01-01T07:42:55.992544Z",
     "iopub.status.idle": "2023-01-01T07:42:56.002054Z",
     "shell.execute_reply.started": "2023-01-01T07:42:55.992509Z",
     "shell.execute_reply": "2023-01-01T07:42:56.001132Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'linux'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# if you're not using gpu to run the machine then it nvidia-smi won't work\n",
    "!nvidia-smi\n",
    "!python --version"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TK5lTmB2NYIK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543815531,
     "user_tz": -60,
     "elapsed": 1012,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "2209091d-97d4-4a92-9e9d-d36aadf19024",
    "execution": {
     "iopub.status.busy": "2023-01-01T08:24:07.959648Z",
     "iopub.execute_input": "2023-01-01T08:24:07.960012Z",
     "iopub.status.idle": "2023-01-01T08:24:09.938293Z",
     "shell.execute_reply.started": "2023-01-01T08:24:07.959980Z",
     "shell.execute_reply": "2023-01-01T08:24:09.937114Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "Sun Jan  1 08:24:08 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 470.82.01    Driver Version: 470.82.01    CUDA Version: 11.4     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0    35W / 250W |    769MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\nPython 3.7.12\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Device\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# paths\n",
    "PATH_TRAINING = \"checkpoints\"\n",
    "DATA_PATH = \"data_mnist\"\n",
    "LOGGING_PATH = \"logging_info\"\n",
    "if not os.path.exists(PATH_TRAINING):\n",
    "    os.makedirs(PATH_TRAINING)\n",
    "if not os.path.exists(LOGGING_PATH):\n",
    "    os.makedirs(LOGGING_PATH)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AdhfgRB9PAie",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543821776,
     "user_tz": -60,
     "elapsed": 169,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "0ebe4bef-fa0e-417f-cd0b-315403f2df86",
    "execution": {
     "iopub.status.busy": "2023-01-01T08:24:11.445493Z",
     "iopub.execute_input": "2023-01-01T08:24:11.445866Z",
     "iopub.status.idle": "2023-01-01T08:24:11.454083Z",
     "shell.execute_reply.started": "2023-01-01T08:24:11.445833Z",
     "shell.execute_reply": "2023-01-01T08:24:11.453017Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "Device: cuda:0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, test_loader, params, train_settings, state):\n",
    "    # number of steps or epoch not given in the paper so have to experiment with\n",
    "    # 60k training data from MNIST, 100 mini batches\n",
    "    training_losses = []\n",
    "    datapoint_training_losses = []\n",
    "    test_losses = []\n",
    "    datapoint_test_losses = []\n",
    "\n",
    "    if train_settings.load_checkpoint:\n",
    "        datapoint_training_losses = state['datapoint_training_losses']\n",
    "        training_losses = state['training_losses']\n",
    "        datapoint_test_losses = state['datapoint_test_losses']\n",
    "        test_losses = state['test_losses']\n",
    "\n",
    "    logging.info(\"Start training...\")\n",
    "    for epoch in range(train_settings.start_epoch, params.NUM_EPOCHS+1):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "\n",
    "        for i, (x, _) in enumerate(train_loader):\n",
    "            # Forward and back prop\n",
    "            x = x.to(DEVICE)\n",
    "            _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "\n",
    "            loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # update model parameters\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # save model checkpoints\n",
    "            if epoch % train_settings.save_rate_epoch == 0 and i % train_settings.save_rate_iter == 0 and train_settings.save_checkpoint:\n",
    "                state = {\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'datapoint_training_losses': datapoint_training_losses,\n",
    "                    'training_losses': training_losses,\n",
    "                    'datapoint_test_losses': datapoint_test_losses,\n",
    "                    'test_losses': test_losses,\n",
    "                    # the loss may not be needed since we start over from a new epoch when resuming\n",
    "                }\n",
    "                # logging.info(f\"Saving model checkpoint at epoch: {epoch}, iter: {i}\")\n",
    "                # logging.info(f\"state_dict: {model.state_dict()}, optimizer: {optimizer.state_dict()}, loss: {loss}\")\n",
    "                torch.save(state, os.path.join(PATH_TRAINING, f'checkpoint_epoch_{epoch}_iter_{i}.pt'))\n",
    "\n",
    "            if i % train_settings.track_rate == 0:\n",
    "                logging.info('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, i * len(x), len(train_loader.dataset),\n",
    "                    100. * i / len(train_loader),\n",
    "                    loss.item() / len(x)))\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    epoch, i * len(x), len(train_loader.dataset),\n",
    "                    100. * i / len(train_loader),\n",
    "                    loss.item() / len(x)))\n",
    "\n",
    "            datapoint_training_losses.append(loss.item() / len(x))\n",
    "\n",
    "            datapoint_test_loss = eval(model, test_loader, loss_fn, True)\n",
    "            datapoint_test_losses.append(datapoint_test_loss)\n",
    "\n",
    "        training_loss = running_loss / len(train_loader.dataset)\n",
    "        training_losses.append(training_loss)\n",
    "        logging.info('====> Epoch: {} Average training loss: {:.4f}'.format(\n",
    "            epoch, training_loss))\n",
    "\n",
    "        test_loss = eval(model, test_loader, loss_fn, None)\n",
    "        test_losses.append(test_loss)\n",
    "        logging.info('====> Epoch: {} Average testing loss: {:.4f}'.format(\n",
    "            epoch, test_loss))\n",
    "\n",
    "    logging.info(f\"params: {str(params.__dict__)}, train settings: {str(train_settings.__dict__)}\")\n",
    "\n",
    "    return training_losses, datapoint_training_losses, test_losses, datapoint_test_losses\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval(model, test_loader, loss_fn, stop=None):\n",
    "    \"\"\"\n",
    "    Train on the test dataset\n",
    "    Can be used to test model after one epoch\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    test_losses = []\n",
    "    datapoint_test_losses = []\n",
    "    running_loss = 0\n",
    "\n",
    "    if not stop:  # evaluate model on whole test dataset\n",
    "        for i, (x, _) in enumerate(test_loader):\n",
    "            # Forward and back prop\n",
    "            x = x.to(DEVICE)\n",
    "            _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "\n",
    "            loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "            running_loss += loss.item()\n",
    "        return running_loss / len(test_loader.dataset)\n",
    "\n",
    "    if stop:  # evaluate model on only one batch\n",
    "        it = iter(test_loader)\n",
    "        x, c = next(it)\n",
    "        x = x.to(DEVICE)\n",
    "        _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "\n",
    "        loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "\n",
    "        return loss.item() / len(x)\n",
    "\n",
    "\n",
    "def test_image(model, x, num_epochs, latent_dim, stop=None, loss_fn=None):\n",
    "    if not loss_fn:\n",
    "        loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "    _, z_mean, z_log_var, x_mean, x_log_var, reconstructed_x = model(x)\n",
    "    loss = calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn)\n",
    "    x = x[0]\n",
    "    reconstructed_x = reconstructed_x.detach().numpy().reshape(x.shape)\n",
    "    save_test_image(x, reconstructed_x, loss.item(), num_epochs, latent_dim)\n",
    "\n",
    "\n",
    "def resume_training(model, optimizer, file_name):\n",
    "    state = torch.load(os.path.join(PATH_TRAINING, file_name))\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    epoch = state['epoch']\n",
    "    # logging.info(f\"Loading model checkpoint\")\n",
    "    # logging.info(f\"state_dict: {model.state_dict()}, optimizer: {optimizer.state_dict()}, epoch: {epoch}\")\n",
    "    return epoch, optimizer, model, state\n",
    "\n",
    "\n",
    "def calculate_loss(z_mean, z_log_var, reconstructed_x, x, loss_fn):\n",
    "    # analytical form of -KL(q_fi(z|x) || p_theta(z))\n",
    "    kl_div = -0.5 * torch.sum(1 + z_log_var\n",
    "                              - z_mean ** 2\n",
    "                              - torch.exp(z_log_var))\n",
    "\n",
    "    # There are some motivations as to why we use MSE:\n",
    "    # https://stats.stackexchange.com/questions/347378/variational-autoencoder-why-reconstruction-term-is-same-to-square-loss\n",
    "    # if data is continuous then the decoder and encoder are gaussian according to the paper, we set p(x|z) to gaussian and get the following\n",
    "    # log(P(x | z)) \\propto log[e^(-|x-x'|^2)] \\propto |x-x'|^2\n",
    "    # others use binary cross-entropy which seems to give results closer to the paper\n",
    "    loss_log_likelihood = loss_fn(reconstructed_x, torch.flatten(x, start_dim=1))\n",
    "\n",
    "    return kl_div + loss_log_likelihood\n",
    "\n",
    "\n",
    "def main():\n",
    "    RANDOM_SEED = 123\n",
    "\n",
    "    # ugly solution perhaps, but needed to bind them to self in order to use in-built .__dict__ formatting to logging\n",
    "    class Params:\n",
    "        def __init__(self):\n",
    "            self.LEARNING_RATE = 2e-2\n",
    "            self.BATCH_SIZE = 100\n",
    "            self.NUM_EPOCHS = 1667\n",
    "            self.HIDDEN_DIMEN = 500\n",
    "            self.LATENT_SPACE = 10  # {3,5,10,20,200}\n",
    "\n",
    "    class RunningSettings:\n",
    "        def __init__(self):\n",
    "            self.optimizer = 'adagrad'  # adagrad or adam\n",
    "            self.criterion = 'bce'  # l1, mse or bce\n",
    "            self.hyperparam_search = False\n",
    "            self.train = True\n",
    "            self.plot = True\n",
    "            self.save_checkpoint = True\n",
    "            self.load_checkpoint = False\n",
    "            self.load_path = '' if not self.load_checkpoint else \"checkpoint_epoch_5_iter_599.pt\"\n",
    "            self.logging_filename = 'train.log'  # 'hyperparameter_search1.log'\n",
    "            self.track_rate = 300  # how often to log batch data loss\n",
    "            self.save_rate_epoch = 5  # how often to save model checkpoints per epoch\n",
    "            self.save_rate_iter = 600  # how often to save model checkpoint per batch iterations\n",
    "            self.start_epoch = 1  # where training epochs start, if loaded from checkpoint then it will be higher than 1\n",
    "            self.device = DEVICE\n",
    "\n",
    "    params = Params()\n",
    "    running_settings = RunningSettings()\n",
    "\n",
    "    random.seed(RANDOM_SEED)\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    torch.manual_seed(RANDOM_SEED)\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "    # setup logger so can save logging.infos, call logging.info\n",
    "    utils.set_logger(os.path.join(LOGGING_PATH, running_settings.logging_filename))\n",
    "\n",
    "    train_loader, test_loader = utils.create_dataset(data_path=DATA_PATH, batch_size=params.BATCH_SIZE)\n",
    "    MNIST_shape = train_loader.dataset.data.shape[1:]  # exclude batch dimension\n",
    "\n",
    "    model = utils.create_model(mnist_shape=MNIST_shape, hidden_dimen=params.HIDDEN_DIMEN,\n",
    "                               latent_space=params.LATENT_SPACE, device=DEVICE)\n",
    "    criterion = utils.create_criterion(running_settings.criterion)\n",
    "    optimizer = utils.create_optimizer(model=model, learning_rate=params.LEARNING_RATE, type=running_settings.optimizer)\n",
    "\n",
    "    # load model checkpoint\n",
    "    state = {}\n",
    "    if running_settings.load_checkpoint:\n",
    "        running_settings.start_epoch, optimizer, model, state = resume_training(model, optimizer, running_settings.load_path)\n",
    "\n",
    "    # hyperparameter search\n",
    "    if running_settings.hyperparam_search:\n",
    "        params_to_optimize = {'lr': [0.01, 0.02, 0.1], 'ls': [3, 5, 10, 20, 200]}\n",
    "        best_learning_rates = utils.learning_rate_hyperparam_search(parameters_to_optimize=params_to_optimize, params=params,\n",
    "                                                                    train_fn=train, mnist_shape=MNIST_shape,\n",
    "                                                                    train_loader=train_loader, test_loader=test_loader,\n",
    "                                                                    running_settings=running_settings)\n",
    "        logging.info(f\"Best learning rates for different latent spaces: {str(best_learning_rates)}\")\n",
    "\n",
    "    # train\n",
    "    if running_settings.train:\n",
    "        tic = time()\n",
    "        training_losses, datapoint_training_losses, test_losses, datapoint_test_losses = \\\n",
    "            train(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  train_loader=train_loader,\n",
    "                  test_loader=test_loader,\n",
    "                  loss_fn=criterion,\n",
    "                  params=params,\n",
    "                  train_settings=running_settings,\n",
    "                  state=state)\n",
    "        final_time = time() - tic\n",
    "        logging.info('Done (t={:0.2f}m)'.format(final_time / 60))\n",
    "\n",
    "    # plot evolution of loss along epochs/datapoint\n",
    "    if running_settings.plot:\n",
    "        plot_epoch_losses(training_losses, test_losses, params.LATENT_SPACE, params.NUM_EPOCHS)\n",
    "        plot_datapoint_losses(datapoint_training_losses, datapoint_test_losses, params.LATENT_SPACE, params.NUM_EPOCHS)\n",
    "\n",
    "        # run model on one image to test\n",
    "        # inputs, classes = next(iter(test_loader))\n",
    "        # x = inputs[np.random.randint(len(inputs))]\n",
    "        # test_image(model, x, params.LATENT_SPACE, params.NUM_EPOCHS, stop=None, loss_fn=criterion)\n",
    "        #\n",
    "        # # plot manifold for latent dimension of 2\n",
    "        # plot_manifold(model, DEVICE, n=12)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z8T6wMoXPgJ9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1672543906723,
     "user_tz": -60,
     "elapsed": 287,
     "user": {
      "displayName": "Bartholomew smithereens",
      "userId": "01906696687539210802"
     }
    },
    "outputId": "0f0ba4d0-2890-4661-a2eb-2bbe2eee90d0",
    "execution": {
     "iopub.status.busy": "2023-01-01T08:25:32.234429Z",
     "iopub.execute_input": "2023-01-01T08:25:32.234795Z",
     "iopub.status.idle": "2023-01-01T08:25:32.270211Z",
     "shell.execute_reply.started": "2023-01-01T08:25:32.234766Z",
     "shell.execute_reply": "2023-01-01T08:25:32.269216Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "main()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_upOd2S2eEqm",
    "outputId": "0fd7ae03-e7b4-4f0d-f4e4-e8ec402b08d1",
    "execution": {
     "iopub.status.busy": "2023-01-01T08:25:37.809866Z",
     "iopub.execute_input": "2023-01-01T08:25:37.810225Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": "Train Epoch: 1 [0/60000 (0%)]\tLoss: 668.235547\nTrain Epoch: 1 [30000/60000 (50%)]\tLoss: 153.798896\nTrain Epoch: 2 [0/60000 (0%)]\tLoss: 150.364336\nTrain Epoch: 2 [30000/60000 (50%)]\tLoss: 139.273027\nTrain Epoch: 3 [0/60000 (0%)]\tLoss: 140.170488\nTrain Epoch: 3 [30000/60000 (50%)]\tLoss: 137.360166\nTrain Epoch: 4 [0/60000 (0%)]\tLoss: 137.208477\nTrain Epoch: 4 [30000/60000 (50%)]\tLoss: 138.209043\nTrain Epoch: 5 [0/60000 (0%)]\tLoss: 139.306299\nTrain Epoch: 5 [30000/60000 (50%)]\tLoss: 133.181016\nTrain Epoch: 6 [0/60000 (0%)]\tLoss: 133.662695\nTrain Epoch: 6 [30000/60000 (50%)]\tLoss: 134.411338\nTrain Epoch: 7 [0/60000 (0%)]\tLoss: 129.386387\nTrain Epoch: 7 [30000/60000 (50%)]\tLoss: 131.957461\nTrain Epoch: 8 [0/60000 (0%)]\tLoss: 127.843760\nTrain Epoch: 8 [30000/60000 (50%)]\tLoss: 130.953691\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 128.541836\nTrain Epoch: 9 [30000/60000 (50%)]\tLoss: 126.535898\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 119.020000\nTrain Epoch: 10 [30000/60000 (50%)]\tLoss: 123.405127\nTrain Epoch: 11 [0/60000 (0%)]\tLoss: 120.064102\nTrain Epoch: 11 [30000/60000 (50%)]\tLoss: 119.289629\nTrain Epoch: 12 [0/60000 (0%)]\tLoss: 125.247451\nTrain Epoch: 12 [30000/60000 (50%)]\tLoss: 126.026484\nTrain Epoch: 13 [0/60000 (0%)]\tLoss: 124.031895\nTrain Epoch: 13 [30000/60000 (50%)]\tLoss: 121.102676\nTrain Epoch: 14 [0/60000 (0%)]\tLoss: 118.590527\nTrain Epoch: 14 [30000/60000 (50%)]\tLoss: 124.551836\nTrain Epoch: 15 [0/60000 (0%)]\tLoss: 119.559766\nTrain Epoch: 15 [30000/60000 (50%)]\tLoss: 115.874404\nTrain Epoch: 16 [0/60000 (0%)]\tLoss: 129.580869\nTrain Epoch: 16 [30000/60000 (50%)]\tLoss: 116.663887\nTrain Epoch: 17 [0/60000 (0%)]\tLoss: 121.374551\nTrain Epoch: 17 [30000/60000 (50%)]\tLoss: 118.136074\nTrain Epoch: 18 [0/60000 (0%)]\tLoss: 121.377061\nTrain Epoch: 18 [30000/60000 (50%)]\tLoss: 117.298408\nTrain Epoch: 19 [0/60000 (0%)]\tLoss: 125.905547\nTrain Epoch: 19 [30000/60000 (50%)]\tLoss: 116.438496\nTrain Epoch: 20 [0/60000 (0%)]\tLoss: 112.997285\nTrain Epoch: 20 [30000/60000 (50%)]\tLoss: 117.938213\nTrain Epoch: 21 [0/60000 (0%)]\tLoss: 119.640928\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
